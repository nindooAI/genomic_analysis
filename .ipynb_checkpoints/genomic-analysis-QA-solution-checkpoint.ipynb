{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "Today the biomedical after doing the genetic mapping need to manually search the referencing of the\n",
    "report on thousands of Pubmed articles.\n",
    "What takes a lot of biomedical time and makes it impossible him to impact other lives through his work.\n",
    "\n",
    "# Challenge \n",
    "\n",
    "- How would you help the biomedical with an AI that understands the context of scientific papers and make recommendations from the biomedical question?\n",
    "- What is the best approach / model for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IN OTHER WORDS:** The goal of this project/research is to find out how can I use AI to seek the answer of Biodical query/question based on the content of PubMed article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undestanding the context (basic search)\n",
    "\n",
    "- [Biomedical](https://www.guiadacarreira.com.br/salarios/quanto-ganha-um-biomedico/): research, identifies and classifies microorganisms that cause diseases and develops or improves medicines and vaccines to combat and prevent these diseases.\n",
    "\n",
    "\n",
    "- [Genetic Mapping](https://www.genome.gov/10000715/genetic-mapping-fact-sheet/): \"Among the main goals of the Human Genome Project (HGP) was to develop new, better and cheaper tools to identify new genes and to understand their function. One of these tools is genetic mapping. Genetic mapping - also called linkage mapping - can offer firm evidence that a disease transmitted from parent to child is linked to one or more genes. Mapping also provides clues about which chromosome contains the gene and precisely where the gene lies on that chromosome. Genetic maps have been used successfully to find the gene responsible for relatively rare, single-gene inherited disorders such as cystic fibrosis and Duchenne muscular dystrophy. Genetic maps are also useful in guiding scientists to the many genes that are believed to play a role in the development of more common disorders such as asthma, heart disease, diabetes, cancer, and psychiatric conditions.\"\n",
    "\n",
    "\n",
    "- [Pubmed articles](https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.PubMed_Quick_Start): \"PubMed comprises over 29 million citations for biomedical literature from MEDLINE, life science journals, and online books. PubMed citations and abstracts include the fields of biomedicine and health, covering portions of the life sciences, behavioral sciences, chemical sciences, and bioengineering. PubMed also provides access to additional relevant web sites and links to the other NCBI molecular biology resources. PubMed is a free resource that is developed and maintained by the National Center for Biotechnology Information (NCBI), at the U.S. National Library of Medicine (NLM), located at the National Institutes of Health (NIH).\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame the Problem\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "- Abstracting the problem\n",
    "- Overview of Question Answering\n",
    "- Define and Implement a Simple Solution\n",
    "- Research about state-of-art application/solutions\n",
    "- BERT \n",
    "- Replicate advanced solutions\n",
    "- Further Research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracting the problem\n",
    "\n",
    "First of all, based on the challenge, it’s a Question Answering (QA) problem which inclus many parts of Natural Language Processing field. \n",
    "\n",
    "Nowadays we have pretty good search engines which make a hard job of rank the best papers based on our queris/question and that makes our lifes easier but It is not enough. \n",
    "Lot’s of paper are produces by day and it’s impossible to follow all researches and dicoveries. Based on that a A.I. solution to comprehend these papers (or general text) using a specific and given context is being study and some important progress were achieved. This field is recognized for having a great potential and the biggest challenge of this field is develop a Open-Domain Question Answering solution that enables users to access the knowledge resources in a natural way and get back a relevant and proper response in concise words. The major landmark of Open-Domain QA was the system developed by IBM Research to beat two champions at the game *Jeopardy!*. (For more information: [This is Watson - David A. Ferrucci, IBM Research Division](https://www.ibm.com/developerworks/community/files/app#/file/39ade3ad-8991-465a-826e-bfd39cd1e541))\n",
    "\n",
    "\n",
    "The next quotes demonstrate the importance of this field. \n",
    "\n",
    "\"Though automatic question answering will definitely be a significant advance in the state-of-art information\n",
    "retrieval technology in forthcoming years but still there are many challenging issues that are yet to be resolved. One of the challenging tasks for existing QA systems is to understand the natural language questions correctly and deduce the precise meaning to retrieve exact responses. Improvement in mechanized understanding of questions faces issues like question classification, formulation of right queries, ambiguity resolution, semantic symmetry detection, identification of temporal relationship in complex questions. In the similar way identification of a perfect answer requires proper validation mechanism.\" From [Research and Reviews in Question Answering System](https://www.sciencedirect.com/science/article/pii/S2212017313005409)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Overview of Question Answering\n",
    "\n",
    "[Research and reviews in question answering system](https://pdf.sciencedirectassets.com/282073/1-s2.0-S2212017313X00052/1-s2.0-S2212017313005409/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEAMaCXVzLWVhc3QtMSJHMEUCIQCYQuZVcR1FHzysAZIifXX3W08Nb3RtxZxvQM230hXqxgIgSkDtwodeaUtQDINwwPGCH2cMiSkqNOzt8SmAa%2F%2BX9m4q4wMI3P%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDC43804kG2x4azORfCq3A4JsbjW%2B82Lu9rUQWKTxiTjc1NGz4sqLm01QQz4kfBsgExUhqFbEhxWOU1tay22IAydzDwJIUyYqhRo52KnVT5iOi4O87Alx5XVYtfkyxNGa2vFAKvvm7kjHI7OvCGQZib20x9UR9QxWQ7MaZTeuKXLxLiL6oTH8Oc%2BFEvkIae%2BysnvMQ6Do47wIZUTCLT2ZZPThKA6Ar4xrRguCBoj5wmvx2vmf2c1qtkmKMfFK4pOiZj3bHn8vJ1j%2Bv5vIzL0JPb6Dg4v8wEnvcpDK3BUXHezWZdXnPM2DVqnXjCa9flrhCQ4pVZvyynNvdaCx5srYSxq4f5hTYWQM9ruKLA7kY5Mv5G4Rw9geqABnnwbz0tER7pPxNZLe%2FM2bx49AdeC7Xi92BVPx6cPUSG0QMJ3xtByRPZa1Mlc3C%2BLrp%2FplGwTsEoq5FghkKdwlPMLSunxXI2FG4RACd2lP%2BGXDtf4ZkWsvPBzdADrIn57rEUlpX2FkUflOGgFI%2B%2FLwO%2BXiRGsg6yJsWIbZnJGCb5Xt8GocabHCFq1CzbxE%2BDgBJR8v9r07aKiUkB3KYODCx6KcebPHLW7P012CX0Yw2tSC5gU6tAFcu0p0WDfxAEzOnbDsDVtRevk5KjSPVRAw2v7w5iuK2GJe%2BmdDD4BmieK8nTETO%2BWJWFN4RXa5km6UMiS03ol6RynvClohfnmUjQWX7cqFytF02H%2Fw9aELvGdejmQkfvjRsknVRq1EaC00iKkzfEMV2gb2JEIHx3ICc7W6Up7BmTqAIqSjm3HYiD5EYERlN8Cx4py0ZrxQrXK9uh0YrWH8UTGBlY3bddqii1zQ8Ah1eYB1%2BV8%3D&AWSAccessKeyId=ASIAQ3PHCVTY65ZFRJ3M&Expires=1556132318&Signature=%2FV1EX1yBhflJBSTMaK0ah5aAgNU%3D&hash=d279400a992c15cb51256809feeebcbf24157f5106a54bbc9ca32af2a34a10b0&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S2212017313005409&tid=spdf-91b9ef03-c79a-418d-a91c-0ccbe50c6e8c&sid=88f86df424c5c741a188f3c0d8be46446ac3gxrqa&type=client)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Usually a architecture of Question Answering systems is split into 3 modules:\n",
    "\n",
    "\n",
    "- **Question Processing:** It’s the first step and the goal is to analyse and classify the question using the question inputed. This process avoid ambiguities in the questions. \n",
    "\n",
    "- **Document Processing:** In this part, based on the undestanding form the question, the relevants documents are selected and the best parts of these docuemnts are extracted. \n",
    "\n",
    "- **Answer Processing:** based on the output of the previous step this module find out the answer for the question. It’s the most difficult part of the QA system.\n",
    "\n",
    "\n",
    "\n",
    "Besides the main architecture,  there are differents paradigm for QA systems:\n",
    "\n",
    "- **Information Retrieval QA:** Usage of search engines to retrieve answers and then apply filters and ranking on the recovered passage. ([A literature review on question answering techniques, paradigmsand systems.](https://reader.elsevier.com/reader/sd/pii/S131915781830082X?token=739A25D8B74882F560024B2C778361C686E83EAF8118DA1B64FE5FEF45D338CD36CED3F47351FF73F5BCDF108671DE74))\n",
    "\n",
    "- **Natural Language Processing QA:** Usage of linguistic intuitions and machine learning methods to extract answers from retrieved snippet. ([A literature review on question answering techniques, paradigmsand systems.](https://reader.elsevier.com/reader/sd/pii/S131915781830082X?token=739A25D8B74882F560024B2C778361C686E83EAF8118DA1B64FE5FEF45D338CD36CED3F47351FF73F5BCDF108671DE74))\n",
    "\n",
    "- **Knowledge Base QA:** Find answers from structured data source (a knowledge base) instead of unstructured text. Standard database queries are used in replacement of word-based searches. ([A literature review on question answering techniques, paradigmsand systems.](https://reader.elsevier.com/reader/sd/pii/S131915781830082X?token=739A25D8B74882F560024B2C778361C686E83EAF8118DA1B64FE5FEF45D338CD36CED3F47351FF73F5BCDF108671DE74))\n",
    "\n",
    "- **Hybrid QA:** High performance QA systems make use of as many types of resources as possible, especially with the prevailing popularity of modern search engines and enriching community-contributed knowledge on the web. A hybrid approach is the combination of IR QA, NLP QA and KB QA. The main example of this paradigm is IBM Watson. ([A literature review on question answering techniques, paradigmsand systems.](https://reader.elsevier.com/reader/sd/pii/S131915781830082X?token=739A25D8B74882F560024B2C778361C686E83EAF8118DA1B64FE5FEF45D338CD36CED3F47351FF73F5BCDF108671DE74))\n",
    "\n",
    "\n",
    "So, it’s clear that QA systems combine lot’s of area in computer science and AI. Some of related topics are:\n",
    "\n",
    "- Information Retrieval (IR)\n",
    "- Natural Language Processing (NLP)\n",
    "- Knowledge Representation and Reasoning (KR&R)\n",
    "- Machine Learning (ML)\n",
    "- Search Engine\n",
    "- Machine Reading Comprehension (MRC)\n",
    "\n",
    "\n",
    "**What are being used?**\n",
    "\n",
    "In the last years, Natural Language Process paradigm achieved great developments using neural network to solve Question Answering problems. Recurrent Neural Networks (RNNs), based on Gate Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) units, is being widely used to handle the longer texts required for QA. Improvements like attention mecanisms and memory networks performed the state-of-art solution for Natural Language Processing based QA for a long time until the end of 2018 when new architectures and pre-trained models were released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a simple solution\n",
    "\n",
    "\n",
    "The solution that I can propouse is to replicate the base line models using some open source solution. I’m  going to show the results of QA systems aplied into famous dataset called [bAbI Task Data](https://research.fb.com/downloads/babi/).\n",
    "\n",
    "### About the Dataset bAbI\n",
    "\n",
    "\n",
    "It's a dataset prepared by Facebook and consists of 20 question answering tasks. There are 1000 training/test context-question-answer example for each task. These task are listed below but for further information access [TOWARDS AI-COMPLETE QUESTION ANSWERING: A SET OF PREREQUISITE TOY TASKS, Facebook AI Reaserach](https://arxiv.org/pdf/1502.05698.pdf)\n",
    "\n",
    "- Task 1: Single Supporting Fact  \n",
    "- Task 2: Two Supporting Facts \n",
    "- Task 3: Three Supporting Facts\n",
    "- Task 4: Two Argument Relation \n",
    "- Task 5: Three Argument Relation \n",
    "- Task 6: Yes/No Questions\n",
    "- Task 7: Counting\n",
    "- Task 8: Lists/Sets\n",
    "- Task 9: Simple Negation\n",
    "- Task 10: Indefinite Knowledge \n",
    "- Task 11: Basic Coreference \n",
    "- Task 12: Conjunction\n",
    "- Task 13: Compound Coreference\n",
    "- Task 14: Time Reasoning\n",
    "- Task 15: Basic Deduction\n",
    "- Task 16: Basic Induction\n",
    "- Task 17: Positional Reasoning\n",
    "- Task 18: Size Reasoning\n",
    "- Task 19: Path Finding\n",
    "- Task 20: Agent’s Motivations\n",
    "\n",
    "\n",
    "\n",
    "    NOTE: Each task aims to test a unique aspect of reasoning and is, therefore, geared towards testing a specific capability of QA learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurent Neural Net Solution - Keras Example\n",
    "\n",
    "**ABOUT THE SOLUTION**\n",
    "\n",
    "The code below is a replication from Keras ([babi_rnn](https://github.com/keras-team/keras/blob/master/examples/babi_rnn.py)). It’s possible to implement a simple solution only using Keras library which achieves similar results with those found by the LSMT baseline provided by [Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks](https://arxiv.org/pdf/1502.05698.pdf) given only few samples.\n",
    "\n",
    "For more information about this solution acess: [Smerity - Blog](https://smerity.com/articles/2015/keras_qa.html)\n",
    "**ABOUT MODEL APPLIED - RNN (BASIC)**\n",
    "\n",
    "Two RNN - recurrent.LSTM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Trains two recurrent neural networks based upon a story and a question.\n",
    "The resulting merged vector is then queried to answer a range of bAbI tasks.\n",
    "\n",
    "Task Number                  | FB LSTM Baseline | Keras QA\n",
    "---                          | ---              | ---\n",
    "QA1 - Single Supporting Fact | 50               | 52.1\n",
    "QA2 - Two Supporting Facts   | 20               | 37.0\n",
    "QA3 - Three Supporting Facts | 20               | 20.5\n",
    "QA4 - Two Arg. Relations     | 61               | 62.9\n",
    "QA5 - Three Arg. Relations   | 70               | 61.9\n",
    "QA6 - yes/No Questions       | 48               | 50.7\n",
    "QA7 - Counting               | 49               | 78.9\n",
    "QA8 - Lists/Sets             | 45               | 77.2\n",
    "QA9 - Simple Negation        | 64               | 64.0\n",
    "QA10 - Indefinite Knowledge  | 44               | 47.7\n",
    "QA11 - Basic Coreference     | 72               | 74.9\n",
    "QA12 - Conjunction           | 74               | 76.4\n",
    "QA13 - Compound Coreference  | 94               | 94.4\n",
    "QA14 - Time Reasoning        | 27               | 34.8\n",
    "QA15 - Basic Deduction       | 21               | 32.4\n",
    "QA16 - Basic Induction       | 23               | 50.6\n",
    "QA17 - Positional Reasoning  | 51               | 49.1\n",
    "QA18 - Size Reasoning        | 52               | 90.8\n",
    "QA19 - Path Finding          | 8                | 9.0\n",
    "QA20 - Agent's Motivations   | 91               | 90.7\n",
    "For the resources related to the bAbI project, refer to:\n",
    "https://research.facebook.com/researchers/1543934539189348\n",
    "### Notes\n",
    "- With default word, sentence, and query vector sizes, the GRU model achieves:\n",
    "  - 52.1% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n",
    "  - 37.0% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\n",
    "In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n",
    "- The task does not traditionally parse the question separately. This likely\n",
    "improves accuracy and is a good example of merging two RNNs.\n",
    "- The word vector embeddings are not shared between the story and question RNNs.\n",
    "- See how the accuracy changes given 10,000 training samples (en-10k) instead\n",
    "of only 1000. 1000 was used in order to be comparable to the original paper.\n",
    "- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\n",
    "- The length and noise (i.e. 'useless' story components) impact the ability of\n",
    "LSTMs / GRUs to provide the correct answer. Given only the supporting facts,\n",
    "these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural\n",
    "networks that use attentional processes can efficiently search through this\n",
    "noise to find the relevant statements, improving performance substantially.\n",
    "This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true,\n",
    "    only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return (pad_sequences(xs, maxlen=story_maxlen),\n",
    "            pad_sequences(xqs, maxlen=query_maxlen), np.array(ys))\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "# Default QA1 with 1000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "# QA1 with 10,000 samples\n",
    "#challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "# QA2 with 1000 samples\n",
    "#challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n",
    "# QA2 with 10,000 samples\n",
    "challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
    "with tarfile.open(path) as tar:\n",
    "    train = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train + test:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n",
    "encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence)\n",
    "\n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n",
    "encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question)\n",
    "\n",
    "merged = layers.concatenate([encoded_sentence, encoded_question])\n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged)\n",
    "\n",
    "model = Model([sentence, question], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "\n",
    "print('Evaluation')\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Network Solution - Keras Example\n",
    "\n",
    "**ABOUT THE SOLUTION**\n",
    "\n",
    "The code below is a replication from Keras ([babi_memnn](https://github.com/keras-team/keras/blob/master/examples/babi_memnn.py)). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It’s possible to implement a simple solution only using Keras library which achieves similar results with those found by the LSMT baseline provided by [Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks](https://arxiv.org/pdf/1502.05698.pdf) given only few samples.\n",
    "\n",
    "\n",
    "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.([End-To-End Memory Networks - Facebook AI Reaerach](https://arxiv.org/pdf/1503.08895.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Trains a memory network on the bAbI dataset.\n",
    "References:\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  [\"Towards AI-Complete Question Answering:\n",
    "  A Set of Prerequisite Toy Tasks\"](http://arxiv.org/abs/1502.05698)\n",
    "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  [\"End-To-End Memory Networks\"](http://arxiv.org/abs/1503.08895)\n",
    "Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n",
    "Time per epoch: 3s on CPU (core i7).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_'\n",
    "                                  'single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_'\n",
    "                                'two-supporting-facts_{}.txt',\n",
    "}\n",
    "challenge_type = 'single_supporting_fact_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "with tarfile.open(path) as tar:\n",
    "    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "# compute a 'match' between the first input vector sequence\n",
    "# and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "# we choose to use a RNN instead.\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "# one regularization layer -- more would probably be needed.\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=120,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-of-the-art QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN have in the last years become the most used architecture for several task which involves language undestanding. As it requires sequential processing, it have to make huge amount of steps when the decision depends on words far away and the number of operations increses if the distance is longer. It also can not use the full power of the modern computing devices which use parallel processing. Based on these problems, the Google Research team propose in June, 2017 a new simple architecture based on self-attention mechanism called **Transformer** where the number of operations is constant and allows parralel inputs.Attention allows the model to focus on the relevant parts of the input sequence as needed. This new architecture was present in the paper: [Attention Is All You Need,  Google Brain and Google Research](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "\n",
    "\n",
    "After this paper, a transformer-XL architecture was released based on the original trasnfomer and it allows longer sequences of inputs to be precessed at one time. So, the input sequence can be break into natural language bondaries and imporve the understanding of deeper context. \n",
    "\n",
    "Based on that Transformer architecture open a new trend in NLP because it enables  people to train models on larger dataset (previously impossible). Furthermore, as it is easier to use these models, much more reusable open-source models are being released. Thus, using transfer learning is much more easier to apply pre-trained models into differents applications like QA. \n",
    "\n",
    "\n",
    "Let’s undestand a little bit more of this new architecture.\n",
    "\n",
    "[Trends in Deep learning NLP](https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transofmer - New Architecture \n",
    "\n",
    "\n",
    "A Transformer consists of a stack of Encoders, a stack of Decoders and a conection between them. See the image below:\n",
    "<img src=\"images/transformer1.png\">([Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/))\n",
    "\n",
    "\n",
    "The encoder's first apply a self-attention layer, a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. The output of this layer is the input of a feed forward neural network. The decoders has these same layers but between them there is an attention layers that makes the decoder focus on the important parts of the inputs. All these layers are illustrated below:\n",
    "\n",
    "<img src=\"images/transformer2.png\">([Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/))\n",
    "\n",
    "**Why self-attention is important?**\n",
    "\n",
    "\"Say the following sentence is an input sentence we want to translate:\n",
    "\n",
    "*'The animal didn't cross the street because it was too tired'*\n",
    "\n",
    "What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\n",
    "\n",
    "When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”. As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\" ([Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/))\n",
    "\n",
    "The first step is to create 3 vectors (Queries, Keys, and Values) that are useful abstractions for determine the attention score. The score is determined using these vectors and it represents how much focus to place on other parts of the input sentence as we encode a word at a certain position. After some math, the score is multiply by the Values vector and the resulting vectorcan be sent to the feed forward neural network.\n",
    "\n",
    "\"The output of the top encoder is then transformed into a set of attention vectors Keys and Values. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.\n",
    "\n",
    "In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
    "\n",
    "The 'Encoder-Decoder Attention' layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\"([Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/))\n",
    "\n",
    "The full architecture can be visualized:\n",
    "\n",
    "<img src=\"images/transformer3.png\">([Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/))\n",
    "\n",
    "\n",
    "The output of the Decoder is vector of floats and it can be transformed to a word applying Linear and Solfmax Layers. The linear layer project the vector outputed from decoder into a logits vector where each cell corresponds to the score of a unique word. Thus, the Solfmax function is applied and the score are transformed into probabilities and the cell(word) with the high probability is chosen. see the illustration below:\n",
    "\n",
    "<img src=\"images/transformer4.png\">([Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **NOTE:** For more information about Transformer access: [Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) and [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenges of the field\n",
    "\n",
    "The challenges of the field can prove the impact of the the Transformer architecture. The most famous ones are listed bellow.\n",
    "\n",
    "- [Natural Questions Challenge - Google AI](https://ai.google.com/research/NaturalQuestions)\n",
    "\n",
    "- [A Conversational Question Answering (CoQA)](https://stanfordnlp.github.io/coqa/)\n",
    "\n",
    "- [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "\n",
    "- [eneral Language Understanding Evaluation (GLUE) benchmark](https://gluebenchmark.com/)\n",
    "\n",
    "- [Question Answering in Context (QuAC)](https://quac.ai/)\n",
    "\n",
    "\n",
    "As we can see, is all of these challenges the leaderboars models uses **BERT** model. It’s a Bidirectional Encoder Representations from Transformers (BERT). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductiong BERT\n",
    "\n",
    "As was discussed before, the training step usually was one of the bigest challenge of NLP, because RNN (previously most used) are computing expensive because these tasks requires a huge amount of data. So, researchers have developed a variety of techniques for training general purpose language representation models using the enormous amount of text (data), known as *pre-training*. \n",
    "Pretrained word embeddings are considered to be an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning). Based on that Google Research relese in November, 2018 a open-source pre-trained models Bidirectional Encoder Representations from Transformers (BERT). This model requires a small amount of data to be tine-tuned (less time consuming) and achieve state-of-the-art results (leaderboar of the challenges prove this fact).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Why BERT?\n",
    "\n",
    "The quote below can explain why is it so important:\n",
    "\n",
    "\n",
    "\"BERT is one such pre-trained model developed by Google which can be fine-tuned on new data which can be used to create NLP systems like question answering, text generation, text classification, text summarization and sentiment analysis. As BERT is trained on huge amount of data, it makes the process of language modeling easier. The main benefit for using pre-trained model of BERT is achievment in substantial accuracy improvements compared to training on these datasets from scratch.\n",
    "\n",
    "BERT builds upon recent work in pre-training contextual representations, it is the first **deeply bidirectional**, unsupervised language representation, pre-trained using only a plain text corpus. BERT represents Contextual representation with both left context and right. BERT is conceptually simple and empirically powerful. BERT is better than previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP having features of Domain Adaptation. As per the BERT paper it can be established that, with proper language model training method, the Transformer(self-attention) based encoder could be potentially used as an alternative to the previous language models.\"([Question Answering System in Python using BERT NLP](https://www.pragnakalp.com/case-study/question-answering-system-in-python-using-bert-nlp/))\n",
    "\n",
    "In other words, this model can read a sentence and return a contextualized embedding for each token which can be\n",
    "used in task-specific neural architectures, like QA.\n",
    "\n",
    "\n",
    "### Pre-training Task\n",
    "\n",
    "BERT was pre-trained using two unsupervised task:\n",
    "\n",
    "\n",
    "**TASK 1 - Mask Language Model**\n",
    "\n",
    "\"In order to train a deep bidirectional representation, we take a straightforward approach of masking some percentage of the input tokens at random, and then predicting only those masked tokens. We refer to this procedure as a “Masked Language Model” (MLM). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard Language Model. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n",
    "(...)\n",
    "The Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token.\"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "\n",
    "**TASK 2 - Next Sentence Prediction**\n",
    "\n",
    "\"In order to train a model that understands sentence relationships, we pre-train a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A, and 50% of the time it is a random sentence from the corpus. For example: \n",
    "\n",
    "Input = *[CLS] the man went to [MASK] store [SEP]*\n",
    "        *he bought a gallon [MASK] milk [SEP]*\n",
    "Label = IsNext\n",
    "\n",
    "\n",
    "Input = *[CLS] the man [MASK] to the store [SEP]*\n",
    "        *penguin [MASK] are flight ##less birds [SEP]*\n",
    "Label = NotNext\n",
    "\n",
    "\n",
    "We choose the NotNext sentences completely at random, and the final pre-trained model achieves 97%-98% accuracy at this task.\n",
    "The pretraining towards this task is very beneficial to both Question Answering and Natural Language Inference.\"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "\n",
    "### Pre-training Procedure \n",
    "\n",
    "- For the pre-training corpous was used the concatenation of BooksCorpus (800M words) and thetext passages of English Wikipedia (2,500M words)\n",
    "\n",
    "\"To generate each training input sequence, we sample two spans of text from the corpus, which we refer to as 'sentences' even though they are typically much longer than single sentences (but can be shorter also). The first sentence receives the A embedding and the second receives the B embedding. 50% of the time B is the actual next\n",
    "sentence that follows A and 50% of the time it is a random sentence, which is done for the “next\n",
    "sentence prediction” task. They are sampled such that the combined length is ≤ 512 tokens. The\n",
    "LM masking is applied after WordPiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces. We train with batch size of 256 sequences (256\n",
    "sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40\n",
    "epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, β1 = 0.9,\n",
    "β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear\n",
    "decay of the learning rate. We use a dropout probability of 0.1 on all layers. We use a gelu activation rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and mean next sentence prediction likelihood.\" [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate Advanced Solution\n",
    "\n",
    "- Google BERT applied on SQuAD Challenge\n",
    "- BioBERT applied on SQuAD challenge\n",
    "- SciBERT research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google BERT Fine-Tuning on Stanford Question Answering Challenge  (SQuAD v1.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The data is a collection of 100k question/answer pair. For example:\n",
    "\n",
    "**Input Question:** *Where do water droplets collide with ice crystals to form precipitation?*\n",
    "\n",
    "\n",
    "**Input Paragraph:** *... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. ...*\n",
    "\n",
    "\n",
    "**Output Answer:** *within a cloud*\n",
    "\n",
    "BERT can be adapt to be applied on the SQuAD. The image represent the application:\n",
    "\n",
    "<img src=\"images/BERT-squad1.png\">\n",
    "\n",
    "* The input is represented as a single package sequence where wuestion and answer use differents embeddings.\n",
    "* The start and end vector are the only 2 parameters fine-tuned.\n",
    "* The probability of a word being the start or end of the answer is computed as a dot product between the respective inputed token and the start or end vector followed by a solfmax function over all the words in the paragraph. The maximum score span is used as prediction and the training optimise this score.\n",
    "* The tokenized labeled span is aligned back to the original untokenized input for evaluation.\n",
    "\n",
    "The model submitted on this challenge from the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) outperforms the previous leaderborad of this challenge.\n",
    "\n",
    "\n",
    "**PROCEDURE**\n",
    "\n",
    "- Install requeriments\n",
    "- Download the pre-trained model *BERT-base-Uncased* from : [Google BERT Pre-trained Models](https://github.com/google-research/bert#fine-tuning-with-bert)\n",
    "- Clone Google [BERT Github](https://github.com/google-research/bert) repository\n",
    "- Read the *Fine-tuning* sectin of SQuAD\n",
    "- Download the pre-treined models (bert_model.ckpt, voceb.txt, bert_config.json)\n",
    "- Fine-tune to SQuAD v1.1\n",
    "- Download the dataset (*train-v1.1.json*; *dev-v1.1.json*)\n",
    "- Download the evaluate code (*evaluate-v1.1.py*)\n",
    "- Run the file *bert-squad11.sh* to train e test the model\n",
    "- The dev set predictions will be saved into a file called *predictions.json* in the output_dir and the follow code will evaluate the predictions:\n",
    "\n",
    "        python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ./squad/predictions.json\n",
    "\n",
    "- The output should be like this: *{\"f1\": 88.41249612335034, \"exact_match\": 81.2488174077578}*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**NOTE:** **I couldn't run this on my ouw CPU. Probably runing on AWS or other Cloud it would be done relatively fast.  Apparently the code is running without erros.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioASQ Challenge\n",
    "\n",
    "\"BioASQ organizes challenges on biomedical semantic indexing and question answering (QA). The challenges include tasks relevant to hierarchical text classification, machine learning, information retrieval, QA from texts and structured data, multi-document summarization and many other areas.\"([BioASQ](http://bioasq.org/))\n",
    "\n",
    "**BioASQ Task B - On Biomedical Semantic QA (Involves IR, QA, Summarization And More)**\n",
    "\n",
    "\"This task uses benchmark datasets containing development and test questions, in English, along with gold standard (reference) answers constructed by a team of biomedical experts. The participants have to respond with relevant concepts, articles, snippets and RDF triples, from designated resources, as well as exact and 'ideal' answers.\"([BioASQ](http://bioasq.org/))\n",
    "\n",
    "Link of the chalenge: [BioASQ](http://bioasq.org/)\n",
    "\n",
    "### BioBERT \n",
    "\n",
    "\"BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain specific language representation model pre-trained on large-scale biomedical corpora. Based on the BERT architecture, BioBERT effectively transfers the knowledge from a large amount of biomedical texts to biomedical text mining models with minimal task-specific architecture modifications.\"[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)\n",
    "\n",
    "Overview of the pre-training and fine-tuning of BioBERT:\n",
    "\n",
    "<img src=\"images/BioBERT.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### PROCEDURE \n",
    "\n",
    "- Install requeriments\n",
    "- Download the pre-trained model: [BioBERT Models - releases](https://github.com/naver/biobert-pretrained/releases)\n",
    "- Clone [dmis-lab/biobert Github](https://github.com/dmis-lab/biobert) repository\n",
    "- Read the *Fine-tuning* section of [dmis-lab/biobert - Github](https://github.com/dmis-lab/biobert)\n",
    "- Download the dataset [BioASQ Task B](http://participants-area.bioasq.org/Tasks/A/getData/)\n",
    "- Run the file *BioBERT-BioASQ.sh* on Terminal to run the train and test\n",
    "- Convert the prediction to the BioASQ JSON format run the follow code on Terminal: \n",
    "\n",
    "        python ./biocodes/transform_nbset2bioasqform.py --nbest_path={QA_output_dir}/nbest_predictions.json --output_path={output_dir}\n",
    "        \n",
    "- Run the evaluation code from [BioASQ GitHub](https://github.com/BioASQ/Evaluation-Measures)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**NOTE:** **I couldn't run this on my ouw CPU. Probably runing on AWS or other Cloud it would be done relatively fast.  Apparently the code is running without erros.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SciBERT - Pretrained Contextualized Embeddings for Scientific Text\n",
    "\n",
    "\n",
    "Although BERT is a excellent pre-trained model which can be used in differents NLP task, it was trained on a general domain corpora. So, SciBERT create a pre-trained contextualized embedding model that follows tha same architecture as BERT but it’s trained on scientific domain, reaching better results than BERT on NLP tasks in this area.\n",
    "\n",
    "- As general and scientific domain use some different words, the Sentence Piece library was used to build a new WordPiece vocabulary, *SciVocab*, on scientific  collections.\n",
    "- SciBERT was trained on random sample of 1.14M papers from Seamntic Scholar. It’s a coomputer science (18%) and biomedical (82%) domain.\n",
    "\n",
    "\n",
    "GitHub link: [SciBERT - GitHub](https://github.com/allenai/scibert)\n",
    "\n",
    "\n",
    "Paper Link: [SCIBERT: Pretrained Contextualized Embeddings for Scientific Text, Allen Institute for Artificial Intelligence](https://arxiv.org/pdf/1903.10676.pdf)\n",
    "\n",
    "**PROBLEM** \n",
    "\n",
    "SciBERT provides a vocab.txt and weights. How can I use these files to update the checkpoint from Google's pre-traindes models and after that perform fine-tune on BioASQ or SQuAD??\n",
    "\n",
    "\n",
    "- **NOTE:** I didn't figure out how can I apply this pre-trained method to replicated good results in QA problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Research\n",
    "\n",
    "### Other great models\n",
    "\n",
    "**OpenAI's Models***\n",
    "\n",
    "- [GPT Model](https://openai.com/blog/language-unsupervised/) and [GPT Github](https://github.com/openai/finetune-transformer-lm)\n",
    "\n",
    "\n",
    "- [GPT-2 Model](https://openai.com/blog/better-language-models/) and [GPT-2 Github](https://github.com/openai/gpt-2)\n",
    "\n",
    "\n",
    "\"When applying fine-tuning based approaches to token-level tasks such as SQuAD question-answering, it is crucial to incorporate context from both directions while with OpenAI GPT, it uses a left-to-right architecture, where every token can only be attended to previous tokens in the self-attention layers of the Transformer.\n",
    "\n",
    "- GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words).\n",
    "- GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only introduced at a fine-tuning time.\n",
    "- BERT learns [SEP](special token), [CLS](classifier token) and sentence embeddings throughout pre-training.\n",
    "- GPT used a similar learning rate of 5e-5 for all fine-tuning experiments. BERT chooses a task-specific fine-tuning learning rate that performs the most effective on the development set.\"([Question Answering System in Python using BERT NLP](https://www.pragnakalp.com/case-study/question-answering-system-in-python-using-bert-nlp/))\n",
    "\n",
    "### Application Demo\n",
    "\n",
    "- Check out these \n",
    "web application Demo: \n",
    "\n",
    "[Deep Learning Analytics Demo](https://deeplearninganalytics.org/demos)\n",
    "\n",
    "[Keras QA Web Api](https://github.com/chen0040/keras-question-and-answering-web-api)\n",
    "\n",
    "An application like this for the Biomedical domain would help be applied to help Biomedicals undestand the articles.\n",
    "\n",
    "### API's\n",
    "\n",
    "- [Best API for NLP](https://blog.rapidapi.com/best-nlp-api/)\n",
    "\n",
    "**IBM Watson Solution**\n",
    "\n",
    "[Create a Natural Language Question Answering system with IBM Watson ](https://fartashh.github.io/post/qa-system-watson/)\n",
    "\n",
    "[Can i build auto Question Answering system using the IBM watson and my QA pair dataset](https://developer.ibm.com/answers/questions/387001/can-i-build-auto-question-answering-system-using-t/)\n",
    "\n",
    "[8 steps for developing question answer solutions using Watson Conversation and Watson Discovery](https://developer.ibm.com/dwblog/2017/best-practices-developing-question-answer-solutions-watson-conversation-discovery/)\n",
    "\n",
    "\n",
    "### Reaserach Groups\n",
    "\n",
    "- [Deep Learning - Machine Reading Comprehension, Microsoft](https://www.microsoft.com/en-us/research/project/deep-learning-machine-reading-comprehension/)\n",
    "- [Google AI Research](https://ai.googleblog.com/)\n",
    "- [OpenAI](https://openai.com/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Other References\n",
    "\n",
    "- [Teach Machine to Comprehend Text and Answer Question with Tensorflow ](https://hanxiao.github.io/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/)\n",
    "- [Dual Ask-Answer Network for Machine Reading Comprehension](https://arxiv.org/pdf/1809.01997.pdf)\n",
    "\n",
    "\n",
    "- [NLP — Building a Question Answering model ](https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54)\n",
    "- [Github - cs224n-Squad-Project](https://github.com/priya-dwivedi/cs224n-Squad-Project)\n",
    "\n",
    "\n",
    "- [Question Answering - State of art solutions](https://paperswithcode.com/task/question-answering)\n",
    "- [Large-scale Simple Question Answering with Memory Networks](https://paperswithcode.com/paper/large-scale-simple-question-answering-with)\n",
    "\n",
    "\n",
    "- [Simple intent recognition and question answering with DeepPavlov](https://medium.com/deeppavlov/simple-intent-recognition-and-question-answering-with-deeppavlov-c54ccf5339a9)\n",
    "- [Open-domain question answering with DeepPavlov](https://medium.com/deeppavlov/open-domain-question-answering-with-deeppavlov-c665d2ee4d65)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- [question-answering-with-tensorflow](https://www.oreilly.com/ideas/question-answering-with-tensorflow)\n",
    "\n",
    "- [Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflo](https://hanxiao.github.io/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/)\n",
    "\n",
    "\n",
    "\n",
    "- [NLP — Building a Question Answering model ](https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54)\n",
    "- [Github - cs224n-Squad-Project](https://github.com/priya-dwivedi/cs224n-Squad-Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Question Answering Using Deep Learning](https://cs224d.stanford.edu/reports/StrohMathur.pdf)\n",
    "\n",
    "- [TOWARDS AI-COMPLETE QUESTION ANSWERING: A SET OF PREREQUISITE TOY TASKS](https://arxiv.org/pdf/1502.05698.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [LSTM AND GRU](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "\n",
    "- [LSTM Explained](https://skymind.ai/wiki/lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
